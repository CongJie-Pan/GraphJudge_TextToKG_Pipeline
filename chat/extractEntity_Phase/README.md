# Entity Extraction and Text Denoising (ECTD) Pipeline

A modular, testable, and extensible implementation of the GPT-5-mini Entity Extraction and Text Denoising pipeline for classical Chinese text processing.

## ğŸ—ï¸ Architecture Overview

The ECTD pipeline is built with a modular architecture that separates concerns and promotes maintainability:

```
extractEntity_Phase/
â”œâ”€â”€ api/                    # API client layer
â”‚   â””â”€â”€ gpt5mini_client.py # GPT-5-mini API client with caching & rate limiting
â”œâ”€â”€ core/                   # Business logic layer
â”‚   â”œâ”€â”€ entity_extractor.py    # Entity extraction from text
â”‚   â”œâ”€â”€ text_denoiser.py       # Text denoising with entity context
â”‚   â””â”€â”€ pipeline_orchestrator.py # Pipeline coordination
â”œâ”€â”€ models/                 # Data models
â”‚   â”œâ”€â”€ entities.py            # Entity data structures
â”‚   â””â”€â”€ pipeline_state.py      # Pipeline execution state
â”œâ”€â”€ utils/                  # Utility functions
â”‚   â”œâ”€â”€ logger.py              # Logging utilities
â”‚   â””â”€â”€ cache_manager.py       # Response caching
â”œâ”€â”€ tests/                  # Test suite
â”‚   â”œâ”€â”€ test_entity_extractor.py
â”‚   â”œâ”€â”€ test_text_denoiser.py
â”‚   â”œâ”€â”€ test_pipeline_orchestrator.py
â”‚   â””â”€â”€ test_pipeline_integration.py
â””â”€â”€ docs/                   # Documentation
    â””â”€â”€ run_entity_modulize_plan.md
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- OpenAI API key with GPT-5-mini access
- Required Python packages (see requirements below)

### Installation

1. **Clone or navigate to the project directory:**
   ```bash
   cd Miscellaneous/KgGen/GraphJudge/chat/extractEntity_Phase
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

   Or install manually:
   ```bash
   pip install litellm pydantic asyncio aiofiles
   ```

3. **Set up environment variables:**
   ```bash
   # Windows
   set OPENAI_API_KEY=your_api_key_here
   
   # Linux/Mac
   export OPENAI_API_KEY=your_api_key_here
   ```

### Basic Usage

#### 1. Simple Entity Extraction

```python
import asyncio
from core.entity_extractor import EntityExtractor
from api.gpt5mini_client import GPT5MiniClient

async def main():
    # Initialize the API client
    client = GPT5MiniClient()
    
    # Create entity extractor
    extractor = EntityExtractor(client)
    
    # Extract entities from texts
    texts = [
        "è³ˆå¯¶ç‰æ˜¯ã€Šç´…æ¨“å¤¢ã€‹ä¸­çš„ä¸»è¦äººç‰©ï¼Œä»–èˆ‡æ—é»›ç‰æœ‰è‘—æ·±åšçš„æ„Ÿæƒ…ã€‚",
        "è–›å¯¶é‡µæ˜¯é‡‘é™µåäºŒé‡µä¹‹ä¸€ï¼Œå¥¹è°æ˜èƒ½å¹¹ï¼Œæ·±å—è³ˆæ¯å–œæ„›ã€‚"
    ]
    
    results = await extractor.extract_entities_from_texts(texts)
    
    for i, collection in enumerate(results):
        print(f"Text {i+1}: {texts[i]}")
        print(f"Entities: {[e.text for e in collection.entities]}")
        print("---")

# Run the async function
asyncio.run(main())
```

#### 2. Text Denoising with Entity Context

```python
import asyncio
from core.text_denoiser import TextDenoiser
from api.gpt5mini_client import GPT5MiniClient

async def main():
    # Initialize components
    client = GPT5MiniClient()
    denoiser = TextDenoiser(client)
    
    # Denoise texts with entity context
    texts = [
        "è³ˆå¯¶ç‰æ˜¯ã€Šç´…æ¨“å¤¢ã€‹ä¸­çš„ä¸»è¦äººç‰©ï¼Œä»–èˆ‡æ—é»›ç‰æœ‰è‘—æ·±åšçš„æ„Ÿæƒ…ã€‚",
        "è–›å¯¶é‡µæ˜¯é‡‘é™µåäºŒé‡µä¹‹ä¸€ï¼Œå¥¹è°æ˜èƒ½å¹¹ï¼Œæ·±å—è³ˆæ¯å–œæ„›ã€‚"
    ]
    
    entities_list = [
        ["è³ˆå¯¶ç‰", "æ—é»›ç‰", "ç´…æ¨“å¤¢"],
        ["è–›å¯¶é‡µ", "é‡‘é™µåäºŒé‡µ", "è³ˆæ¯"]
    ]
    
    denoised_texts = await denoiser.denoise_texts(texts, entities_list)
    
    for i, (original, denoised) in enumerate(zip(texts, denoised_texts)):
        print(f"Original {i+1}: {original}")
        print(f"Denoised {i+1}: {denoised}")
        print("---")

asyncio.run(main())
```

#### 3. Full Pipeline Execution

```python
import asyncio
from core.pipeline_orchestrator import PipelineOrchestrator, PipelineConfig

async def main():
    # Configure the pipeline
    config = PipelineConfig(
        batch_size=5,
        max_retries=3,
        output_dir="./output",
        enable_logging=True
    )
    
    # Create and run the pipeline
    orchestrator = PipelineOrchestrator(config)
    
    # Option 1: Provide texts directly
    input_texts = [
        "è³ˆå¯¶ç‰æ˜¯ã€Šç´…æ¨“å¤¢ã€‹ä¸­çš„ä¸»è¦äººç‰©ã€‚",
        "æ—é»›ç‰æ˜¯è³ˆå¯¶ç‰çš„è¡¨å¦¹ã€‚",
        "è–›å¯¶é‡µæ˜¯é‡‘é™µåäºŒé‡µä¹‹ä¸€ã€‚"
    ]
    
    success = await orchestrator.run_pipeline(input_texts)
    
    if success:
        print("Pipeline completed successfully!")
        print(f"Statistics: {orchestrator.get_pipeline_statistics()}")
    else:
        print("Pipeline failed. Check logs for details.")

# Run the pipeline
asyncio.run(main())
```

## ğŸ”§ Configuration

### Entity Extractor Configuration

```python
from core.entity_extractor import ExtractionConfig

config = ExtractionConfig(
    batch_size=10,           # Process 10 texts at once
    max_retries=3,           # Retry failed API calls up to 3 times
    use_examples=True,       # Include examples in prompts
    enable_deduplication=True # Remove duplicate entities
)

extractor = EntityExtractor(client, config)
```

### Text Denoiser Configuration

```python
from core.text_denoiser import DenoisingConfig

config = DenoisingConfig(
    batch_size=8,            # Process 8 texts at once
    max_retries=2,           # Retry failed API calls up to 2 times
    use_examples=True,       # Include examples in prompts
    similarity_threshold=0.7  # Minimum similarity for validation
)

denoiser = TextDenoiser(client, config)
```

### Pipeline Configuration

```python
from core.pipeline_orchestrator import PipelineConfig

config = PipelineConfig(
    batch_size=5,            # Overall batch size for pipeline stages
    max_retries=3,           # Maximum retries for any stage
    output_dir="./results",  # Output directory for results
    enable_logging=True,     # Enable detailed logging
    save_intermediate=True   # Save intermediate results
)

orchestrator = PipelineOrchestrator(config)
```

## ğŸ“Š Monitoring and Statistics

### Entity Extraction Statistics

```python
# Get extraction statistics
stats = extractor.get_statistics()
print(f"Processed texts: {stats['total_texts']}")
print(f"Total entities: {stats['total_entities']}")
print(f"Average entities per text: {stats['avg_entities_per_text']}")
print(f"Success rate: {stats['success_rate']:.2%}")

# Reset statistics
extractor.reset_statistics()
```

### Text Denoising Statistics

```python
# Get denoising statistics
stats = denoiser.get_statistics()
print(f"Processed texts: {stats['total_texts']}")
print(f"Average compression ratio: {stats['avg_compression_ratio']:.2%}")
print(f"Success rate: {stats['success_rate']:.2%}")

# Reset statistics
denoiser.reset_statistics()
```

### Pipeline Statistics

```python
# Get overall pipeline statistics
pipeline_stats = orchestrator.get_pipeline_statistics()
print(f"Total execution time: {pipeline_stats['total_execution_time']:.2f}s")
print(f"Entities extracted: {pipeline_stats['total_entities']}")
print(f"Texts denoised: {pipeline_stats['total_texts_denoised']}")

# Get pipeline state
state = orchestrator.get_pipeline_state()
print(f"Current status: {state.status}")
print(f"Current stage: {state.current_stage}")
```

## ğŸ§ª Testing

### Run All Tests

```bash
# Run all tests
python -m pytest tests/ -v

# Run with coverage
python -m pytest tests/ --cov=core --cov=api --cov=models -v

# Run specific test file
python -m pytest tests/test_entity_extractor.py -v
```

### Test Categories

- **Unit Tests**: Test individual components in isolation
- **Integration Tests**: Test component interactions
- **End-to-End Tests**: Test complete pipeline workflows

## ğŸ“ Output Structure

The pipeline generates the following output structure:

```
output/
â”œâ”€â”€ entities_YYYYMMDD_HHMMSS.json      # Extracted entities
â”œâ”€â”€ denoised_texts_YYYYMMDD_HHMMSS.txt # Denoised texts
â”œâ”€â”€ pipeline_stats_YYYYMMDD_HHMMSS.json # Pipeline statistics
â””â”€â”€ pipeline_state_YYYYMMDD_HHMMSS.json # Pipeline execution state
```

## ğŸ” Error Handling

The system provides comprehensive error handling:

```python
try:
    results = await extractor.extract_entities_from_texts(texts)
except Exception as e:
    print(f"Extraction failed: {e}")
    # Check detailed error information
    state = extractor.get_pipeline_state()
    if state.errors:
        for error in state.errors:
            print(f"Error: {error.message} (Severity: {error.severity})")
```

## ğŸš€ Advanced Usage

### Custom Entity Types

```python
from models.entities import EntityType

# The system automatically classifies entities, but you can customize
def custom_classifier(entity_text: str, source_text: str) -> EntityType:
    if "å¯¶ç‰" in entity_text:
        return EntityType.PERSON
    elif "ç´…æ¨“å¤¢" in entity_text:
        return EntityType.WORK
    return EntityType.OTHER

# Use in extractor
extractor._classify_entity_type = custom_classifier
```

### Progress Callbacks

```python
def progress_callback(current: int, total: int, stage: str):
    print(f"{stage}: {current}/{total} ({current/total*100:.1f}%)")

# Use in extraction
results = await extractor.extract_entities_from_texts(
    texts, 
    progress_callback=progress_callback
)
```

### Custom Prompt Templates

```python
# Override prompt building methods
def custom_prompt(text: str) -> str:
    return f"è«‹å¾ä»¥ä¸‹å¤å…¸ä¸­æ–‡æ–‡æœ¬ä¸­æå–å¯¦é«”ï¼š\n\n{text}\n\nè«‹ä»¥JSONæ ¼å¼è¿”å›çµæœã€‚"

extractor._build_extraction_prompt = custom_prompt
```

## ğŸ”§ Troubleshooting

### Common Issues

1. **API Key Issues**
   - Ensure `OPENAI_API_KEY` is set correctly
   - Check API key permissions for GPT-5-mini

2. **Rate Limiting**
   - The system includes built-in rate limiting
   - Adjust batch sizes if hitting API limits

3. **Memory Issues**
   - Reduce batch sizes for large text collections
   - Process texts in smaller chunks

4. **Text Encoding**
   - Ensure texts are properly encoded (UTF-8)
   - Handle special characters appropriately

### Debug Mode

```python
import logging

# Enable debug logging
logging.basicConfig(level=logging.DEBUG)

# Or use the built-in logger
from utils.logger import TerminalLogger
logger = TerminalLogger(level="DEBUG")
```

## ğŸ“š API Reference

### Core Classes

- **`EntityExtractor`**: Main entity extraction logic
- **`TextDenoiser`**: Text denoising with entity context
- **`PipelineOrchestrator`**: Complete pipeline coordination
- **`GPT5MiniClient`**: API client with caching and rate limiting

### Key Methods

- **`extract_entities_from_texts()`**: Extract entities from multiple texts
- **`denoise_texts()`**: Denoise texts using entity context
- **`run_pipeline()`**: Execute complete ECTD pipeline
- **`get_statistics()`**: Retrieve processing statistics

## ğŸ¤ Contributing

When contributing to this module:

1. Follow the existing code structure and patterns
2. Add comprehensive tests for new functionality
3. Update documentation for new features
4. Ensure all tests pass before submitting changes

## ğŸ“„ License

This module is part of the larger project and follows the same licensing terms.

## ğŸ†˜ Support

For issues or questions:

1. Check the troubleshooting section above
2. Review the test files for usage examples
3. Check the original `run_entity.py` for reference implementation
4. Review the modularization plan in `docs/run_entity_modulize_plan.md`
