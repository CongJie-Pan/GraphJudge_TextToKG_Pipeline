"""
Text Denoising Engine Module

This module implements the core text denoising logic for classical Chinese texts,
using GPT-5-mini to restructure text based on extracted entities.

The module provides a clean interface for text denoising with comprehensive
error handling, progress tracking, and result validation.
"""

import asyncio
import json
import re
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass

from extractEntity_Phase.models.entities import Entity, EntityList
from extractEntity_Phase.api.gpt5mini_client import GPT5MiniClient, APIRequest, APIResponse
from extractEntity_Phase.infrastructure.logging import get_logger
from extractEntity_Phase.utils.chinese_text import ChineseTextProcessor
from extractEntity_Phase.models.pipeline_state import PipelineStage, ProcessingStatus


@dataclass
class DenoisingConfig:
    """Configuration for text denoising."""
    
    # GPT-5-mini settings
    temperature: float = 1.0
    max_tokens: int = 4000
    timeout: int = 60
    
    # Denoising settings
    preserve_classical_style: bool = True
    maintain_factual_accuracy: bool = True
    enable_entity_relationships: bool = True
    min_output_length: int = 20
    max_output_length: int = 1000
    
    # Prompt engineering
    use_system_prompt: bool = True
    include_examples: bool = True
    language: str = "zh-TW"  # Traditional Chinese
    
    # Processing settings
    batch_size: int = 10
    max_concurrent: int = 3


class TextDenoiser:
    """
    Core text denoising engine for classical Chinese texts.
    
    This class implements the main logic for denoising and restructuring Chinese text
    using GPT-5-mini, based on extracted entities while preserving classical style.
    """
    
    def __init__(self, client: GPT5MiniClient, config: Optional[DenoisingConfig] = None):
        """
        Initialize the text denoiser.
        
        Args:
            client: GPT-5-mini API client
            config: Denoising configuration
        """
        self.client = client
        self.config = config or DenoisingConfig()
        self.logger = get_logger()
        self.text_processor = ChineseTextProcessor()
        
        # Statistics tracking
        self.stats = {
            "total_texts_processed": 0,
            "total_texts_denoised": 0,
            "successful_denoising": 0,
            "failed_denoising": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "average_compression_ratio": 0.0
        }
    
    async def denoise_texts(
        self, 
        texts: List[str],
        entities_list: List[Union[List[str], EntityList]],
        progress_callback: Optional[callable] = None
    ) -> List[str]:
        """
        Denoise and restructure a list of Chinese texts based on extracted entities.
        
        Args:
            texts: List of original Chinese text strings
            entities_list: List of entity lists or EntityList objects
            progress_callback: Optional callback for progress updates
            
        Returns:
            List of denoised and restructured texts
        """
        self.logger.log(f"üßπ Starting text denoising for {len(texts)} text segments...")
        
        if not texts or not entities_list:
            self.logger.log("‚ö†Ô∏è No texts or entities provided for denoising")
            return []
        
        if len(texts) != len(entities_list):
            self.logger.log("‚ùå Mismatch between number of texts and entity lists")
            return []
        
        # Validate inputs
        validated_pairs = self._validate_input_pairs(texts, entities_list)
        if not validated_pairs:
            self.logger.log("‚ùå Input validation failed")
            return []
        
        # Process texts in batches
        results = []
        total_batches = (len(validated_pairs) + self.config.batch_size - 1) // self.config.batch_size
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * self.config.batch_size
            end_idx = min(start_idx + self.config.batch_size, len(validated_pairs))
            batch_pairs = validated_pairs[start_idx:end_idx]
            
            self.logger.log(f"üì¶ Processing denoising batch {batch_idx + 1}/{total_batches} "
                          f"(texts {start_idx + 1}-{end_idx})")
            
            # Denoise batch
            batch_results = await self._denoise_texts_batch(batch_pairs)
            results.extend(batch_results)
            
            # Update progress
            if progress_callback:
                progress = (batch_idx + 1) / total_batches
                progress_callback(progress, f"Denoised batch {batch_idx + 1}/{total_batches}")
            
            # Add delay between batches to respect rate limits
            if batch_idx < total_batches - 1:
                await asyncio.sleep(0.5)
        
        # Update statistics
        self._update_statistics(texts, results)
        
        self.logger.log(f"‚úÖ Text denoising completed. "
                       f"Processed {len(texts)} texts, "
                       f"successful: {self.stats['successful_denoising']}")
        
        return results
    
    async def _denoise_texts_batch(
        self, 
        text_entity_pairs: List[Tuple[str, Union[List[str], EntityList]]]
    ) -> List[str]:
        """
        Denoise a batch of text-entity pairs.
        
        Args:
            text_entity_pairs: List of (text, entities) tuples
            
        Returns:
            List of denoised texts
        """
        # Create denoising prompts
        prompts = self._create_denoising_prompts(text_entity_pairs)
        
        # Execute API calls
        responses = await self._execute_denoising_requests(prompts)
        
        # Parse and validate responses
        results = []
        for (text, entities), response in zip(text_entity_pairs, responses):
            try:
                denoised_text = self._parse_denoising_response(response, text, entities)
                if denoised_text:
                    results.append(denoised_text)
                else:
                    # Fallback to original text if denoising fails
                    results.append(text)
                    self.logger.log(f"‚ö†Ô∏è Denoising failed for text, using original")
            except Exception as e:
                self.logger.log(f"‚ùå Failed to parse denoising response: {str(e)}")
                # Use original text as fallback
                results.append(text)
        
        return results
    
    def _create_denoising_prompts(
        self, 
        text_entity_pairs: List[Tuple[str, Union[List[str], EntityList]]]
    ) -> List[APIRequest]:
        """
        Create denoising prompts for the given text-entity pairs.
        
        Args:
            text_entity_pairs: List of (text, entities) tuples
            
        Returns:
            List of APIRequest objects
        """
        prompts = []
        
        for text, entities in text_entity_pairs:
            # Convert entities to list of strings if needed
            entity_strings = self._extract_entity_strings(entities)
            
            # Create user prompt
            user_prompt = self._build_denoising_prompt(text, entity_strings)
            
            # Create system prompt if enabled
            system_prompt = None
            if self.config.use_system_prompt:
                system_prompt = self._build_system_prompt()
            
            # Create API request
            request = APIRequest(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens
            )
            prompts.append(request)
        
        return prompts
    
    def _extract_entity_strings(self, entities: Union[List[str], EntityList]) -> List[str]:
        """
        Extract entity strings from various entity formats.
        
        Args:
            entities: Entity list or EntityList object
            
        Returns:
            List of entity strings
        """
        if isinstance(entities, EntityList):
            return [entity.text for entity in entities.entities]
        elif isinstance(entities, list):
            # Handle list of strings or Entity objects
            entity_strings = []
            for entity in entities:
                if isinstance(entity, str):
                    entity_strings.append(entity)
                elif isinstance(entity, Entity):
                    entity_strings.append(entity.text)
                else:
                    entity_strings.append(str(entity))
            return entity_strings
        else:
            return [str(entities)]
    
    def _build_denoising_prompt(self, text: str, entities: List[str]) -> str:
        """
        Build denoising prompt for a single text-entity pair.
        
        Args:
            text: Original text to denoise
            entities: List of extracted entities
            
        Returns:
            Formatted denoising prompt
        """
        if self.config.include_examples:
            prompt = self._build_prompt_with_examples(text, entities)
        else:
            prompt = self._build_simple_prompt(text, entities)
        
        return prompt
    
    def _build_prompt_with_examples(self, text: str, entities: List[str]) -> str:
        """
        Build denoising prompt with comprehensive examples.
        
        Args:
            text: Original text to denoise
            entities: List of extracted entities
            
        Returns:
            Prompt with examples
        """
        return f"""ÁõÆÊ®ôÔºö
Âü∫ÊñºÁµ¶ÂÆöÁöÑÂØ¶È´îÔºåÂ∞çÂè§ÂÖ∏‰∏≠ÊñáÊñáÊú¨ÈÄ≤Ë°åÂéªÂô™ËôïÁêÜÔºåÂç≥ÁßªÈô§ÁÑ°ÈóúÁöÑÊèèËø∞ÊÄßÊñáÂ≠ó‰∏¶ÈáçÁµÑÁÇ∫Ê∏ÖÊô∞ÁöÑ‰∫ãÂØ¶Èô≥Ëø∞„ÄÇ

‰ª•‰∏ãÊòØ„ÄäÁ¥ÖÊ®ìÂ§¢„ÄãÁöÑ‰∏âÂÄãÁØÑ‰æãÔºö
ÁØÑ‰æã#1:
ÂéüÂßãÊñáÊú¨Ôºö"ÂªüÊóÅ‰ΩèËëó‰∏ÄÂÆ∂ÈÑâÂÆ¶ÔºåÂßìÁîÑÔºåÂêçË≤ªÔºåÂ≠óÂ£´Èö±„ÄÇÂ´°Â¶ªÂ∞ÅÊ∞èÔºåÊÉÖÊÄßË≥¢Ê∑ëÔºåÊ∑±ÊòéÁ¶ÆÁæ©„ÄÇÂÆ∂‰∏≠Èõñ‰∏çÁîöÂØåË≤¥ÔºåÁÑ∂Êú¨Âú∞‰æø‰πüÊé®‰ªñÁÇ∫ÊúõÊóè‰∫Ü„ÄÇ"
ÂØ¶È´îÔºö["ÁîÑË≤ª", "ÁîÑÂ£´Èö±", "Â∞ÅÊ∞è", "ÈÑâÂÆ¶"]
ÂéªÂô™ÊñáÊú¨Ôºö"ÁîÑÂ£´Èö±ÊòØ‰∏ÄÂÆ∂ÈÑâÂÆ¶„ÄÇÁîÑÂ£´Èö±ÂßìÁîÑÂêçË≤ªÂ≠óÂ£´Èö±„ÄÇÁîÑÂ£´Èö±ÁöÑÂ¶ªÂ≠êÊòØÂ∞ÅÊ∞è„ÄÇÂ∞ÅÊ∞èÊÉÖÊÄßË≥¢Ê∑ëÊ∑±ÊòéÁ¶ÆÁæ©„ÄÇÁîÑÂÆ∂ÊòØÊú¨Âú∞ÊúõÊóè„ÄÇ"

ÁØÑ‰æã#2:
ÂéüÂßãÊñáÊú¨Ôºö"Ë≥àÈõ®ÊùëÂéüÁ≥ªËÉ°Â∑û‰∫∫Ê∞èÔºå‰πüÊòØË©©Êõ∏‰ªïÂÆ¶‰πãÊóèÔºåÂõ†‰ªñÁîüÊñºÊú´‰∏ñÔºåÁà∂ÊØçÁ•ñÂÆóÊ†πÂü∫Â∑≤Áõ°Ôºå‰∫∫Âè£Ë°∞Âñ™ÔºåÂè™Ââ©Âæó‰ªñ‰∏ÄË∫´‰∏ÄÂè£ÔºåÂú®ÂÆ∂ÈÑâÁÑ°ÁõäÔºåÂõ†ÈÄ≤‰∫¨Ê±ÇÂèñÂäüÂêçÔºåÂÜçÊï¥Âü∫Ê•≠„ÄÇ"
ÂØ¶È´îÔºö["Ë≥àÈõ®Êùë", "ËÉ°Â∑û", "Ë©©Êõ∏‰ªïÂÆ¶‰πãÊóè"]
ÂéªÂô™ÊñáÊú¨Ôºö"Ë≥àÈõ®ÊùëÊòØËÉ°Â∑û‰∫∫Ê∞è„ÄÇË≥àÈõ®ÊùëÊòØË©©Êõ∏‰ªïÂÆ¶‰πãÊóè„ÄÇË≥àÈõ®ÊùëÁîüÊñºÊú´‰∏ñ„ÄÇË≥àÈõ®ÊùëÁà∂ÊØçÁ•ñÂÆóÊ†πÂü∫Â∑≤Áõ°„ÄÇË≥àÈõ®ÊùëÈÄ≤‰∫¨Ê±ÇÂèñÂäüÂêç„ÄÇË≥àÈõ®ÊùëÊÉ≥Ë¶ÅÈáçÊï¥Âü∫Ê•≠„ÄÇ"

ÁØÑ‰æã#3:
ÂéüÂßãÊñáÊú¨Ôºö"Ë≥àÂØ∂ÁéâÂõ†Â§¢ÈÅäÂ§™ËôõÂπªÂ¢ÉÔºåÈ†ìÁîüÁñëÊáºÔºåÈÜí‰æÜÂæåÂøÉ‰∏≠‰∏çÂÆâÔºåÈÅÇÂ∞áÊ≠§‰∫ãÂëäÁü•ÊûóÈªõÁéâÔºåÈªõÁéâËÅΩÂæå‰∫¶ÊÑüÈ©öÁï∞„ÄÇ"
ÂØ¶È´îÔºö["Ë≥àÂØ∂Áéâ", "Â§™ËôõÂπªÂ¢É", "ÊûóÈªõÁéâ"]
ÂéªÂô™ÊñáÊú¨Ôºö"Ë≥àÂØ∂ÁéâÂ§¢ÈÅäÂ§™ËôõÂπªÂ¢É„ÄÇË≥àÂØ∂ÁéâÂ§¢ÈÜíÂæåÈ†ìÁîüÁñëÊáº„ÄÇË≥àÂØ∂ÁéâÂ∞áÊ≠§‰∫ãÂëäÁü•ÊûóÈªõÁéâ„ÄÇÊûóÈªõÁéâËÅΩÂæåÊÑüÂà∞È©öÁï∞„ÄÇ"

Ë´ãÂèÉËÄÉ‰ª•‰∏äÁØÑ‰æãÔºåËôïÁêÜ‰ª•‰∏ãÊñáÊú¨Ôºö
ÂéüÂßãÊñáÊú¨Ôºö{text}
ÂØ¶È´îÔºö{entities}
ÂéªÂô™ÊñáÊú¨Ôºö"""
    
    def _build_simple_prompt(self, text: str, entities: List[str]) -> str:
        """
        Build simple denoising prompt without examples.
        
        Args:
            text: Original text to denoise
            entities: List of extracted entities
            
        Returns:
            Simple denoising prompt
        """
        return f"""Âü∫ÊñºÁµ¶ÂÆöÁöÑÂØ¶È´îÔºåÂ∞ç‰ª•‰∏ãÂè§ÂÖ∏‰∏≠ÊñáÊñáÊú¨ÈÄ≤Ë°åÂéªÂô™ËôïÁêÜÔºå
ÁßªÈô§ÁÑ°ÈóúÁöÑÊèèËø∞ÊÄßÊñáÂ≠ó‰∏¶ÈáçÁµÑÁÇ∫Ê∏ÖÊô∞ÁöÑ‰∫ãÂØ¶Èô≥Ëø∞Ôºö

ÂéüÂßãÊñáÊú¨Ôºö{text}
ÂØ¶È´îÔºö{entities}
ÂéªÂô™ÊñáÊú¨Ôºö"""
    
    def _build_system_prompt(self) -> str:
        """
        Build system prompt for text denoising.
        
        Returns:
            System prompt string
        """
        return """‰Ω†ÊòØ‰∏ÄÂÄãÂ∞àÈñÄËôïÁêÜÂè§ÂÖ∏‰∏≠ÊñáÊñáÊú¨ÁöÑÂéªÂô™Â∞àÂÆ∂„ÄÇË´ãÂö¥Ê†ºÊåâÁÖß‰ª•‰∏ãË¶ÅÊ±ÇÔºö

1. Âü∫ÊñºÁµ¶ÂÆöÁöÑÂØ¶È´îÔºåÂ∞çÊñáÊú¨ÈÄ≤Ë°åÂéªÂô™ËôïÁêÜ
2. ÁßªÈô§ÁÑ°ÈóúÁöÑÊèèËø∞ÊÄßÊñáÂ≠óÂíå‰øÆÈ£æË™û
3. ÈáçÁµÑÁÇ∫Ê∏ÖÊô∞„ÄÅÁ∞°ÊΩîÁöÑ‰∫ãÂØ¶Èô≥Ëø∞
4. ‰øùÊåÅÂè§ÂÖ∏‰∏≠ÊñáÁöÑË™ûË®ÄÈ¢®Ê†ºÂíåÈüªÂë≥
5. Á¢∫‰øùÊØèÂÄãÈô≥Ëø∞ÈÉΩÂü∫ÊñºÁµ¶ÂÆöÁöÑÂØ¶È´î
6. ÈÅøÂÖçÊ∑ªÂä†ÂéüÊñá‰∏≠Ê≤íÊúâÁöÑ‰ø°ÊÅØ
7. ‰ΩøÁî®Á∞°ÊΩîÁöÑÂè•ÂºèÔºåÊØèÂÄã‰∫ãÂØ¶Áî®‰∏ÄÂè•Ë©±Ë°®ÈÅî

Ë´ãÂ∞àÊ≥®ÊñºÊèêÂèñÂíåÈáçÁµÑ‰∫ãÂØ¶‰ø°ÊÅØÔºå‰øùÊåÅÊñáÊú¨ÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÂèØËÆÄÊÄß„ÄÇ"""
    
    async def _execute_denoising_requests(
        self, 
        requests: List[APIRequest]
    ) -> List[APIResponse]:
        """
        Execute text denoising API requests.
        
        Args:
            requests: List of APIRequest objects
            
        Returns:
            List of APIResponse objects
        """
        # Use semaphore to limit concurrent requests
        semaphore = asyncio.Semaphore(self.config.max_concurrent)
        
        async def execute_request(request: APIRequest) -> APIResponse:
            async with semaphore:
                try:
                    response = await self.client.complete(request)
                    
                    # Update cache statistics
                    if response.cached:
                        self.stats["cache_hits"] += 1
                    else:
                        self.stats["cache_misses"] += 1
                    
                    return response
                except Exception as e:
                    self.logger.log(f"‚ùå API request failed: {str(e)}")
                    # Return error response
                    return APIResponse(
                        content=f"Error: {str(e)}",
                        model="gpt-5-mini",
                        usage={"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                        finish_reason="error",
                        response_time=0.0,
                        error=str(e)
                    )
        
        # Execute requests concurrently
        tasks = [execute_request(req) for req in requests]
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Handle exceptions
        processed_responses = []
        for i, response in enumerate(responses):
            if isinstance(response, Exception):
                self.logger.log(f"‚ùå Request {i} failed with exception: {str(response)}")
                processed_responses.append(APIResponse(
                    content=f"Error: {str(response)}",
                    model="gpt-5-mini",
                    usage={"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                    finish_reason="error",
                    response_time=0.0,
                    error=str(response)
                ))
            else:
                processed_responses.append(response)
        
        return processed_responses
    
    def _parse_denoising_response(
        self, 
        response: APIResponse, 
        original_text: str, 
        entities: Union[List[str], EntityList]
    ) -> Optional[str]:
        """
        Parse text denoising response from GPT-5-mini.
        
        Args:
            response: API response from GPT-5-mini
            original_text: Original source text
            entities: Extracted entities used for denoising
            
        Returns:
            Denoised text string or None if parsing fails
            
        Raises:
            ValueError: If response parsing fails
        """
        if response.error:
            raise ValueError(f"API response error: {response.error}")
        
        content = response.content.strip()
        if not content:
            raise ValueError("Empty response content")
        
        # Clean and validate denoised text
        denoised_text = self._clean_denoised_text(content)
        
        # Validate denoised text quality
        if not self._validate_denoised_text(denoised_text, original_text, entities):
            self.logger.log(f"‚ö†Ô∏è Denoised text validation failed, using original")
            return None
        
        return denoised_text
    
    def _clean_denoised_text(self, content: str) -> str:
        """
        Clean and format denoised text.
        
        Args:
            content: Raw denoised text content
            
        Returns:
            Cleaned denoised text
        """
        # Remove common prefixes and suffixes
        prefixes_to_remove = [
            "ÂéªÂô™ÊñáÊú¨Ôºö", "Denoised text:", "Result:", "Answer:", "A:"
        ]
        suffixes_to_remove = [
            "„ÄÇ", ".", "ÔºÅ", "ÔºÅ", "Ôºü", "?", "\n", "\r"
        ]
        
        cleaned_text = content.strip()
        
        # Remove prefixes
        for prefix in prefixes_to_remove:
            if cleaned_text.startswith(prefix):
                cleaned_text = cleaned_text[len(prefix):].strip()
        
        # Remove trailing punctuation and whitespace
        for suffix in suffixes_to_remove:
            if cleaned_text.endswith(suffix):
                cleaned_text = cleaned_text[:-len(suffix)].strip()
        
        # Normalize whitespace
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
        
        return cleaned_text.strip()
    
    def _validate_denoised_text(
        self, 
        denoised_text: str, 
        original_text: str, 
        entities: Union[List[str], EntityList]
    ) -> bool:
        """
        Validate the quality of denoised text.
        
        Args:
            denoised_text: Denoised text to validate
            original_text: Original source text
            entities: Extracted entities
            
        Returns:
            True if denoised text is valid, False otherwise
        """
        if not denoised_text or len(denoised_text.strip()) < self.config.min_output_length:
            return False
        
        if len(denoised_text) > self.config.max_output_length:
            return False
        
        # Check if denoised text contains key entities
        entity_strings = self._extract_entity_strings(entities)
        entity_coverage = sum(1 for entity in entity_strings if entity in denoised_text)
        
        if entity_coverage < max(1, len(entity_strings) * 0.5):  # At least 50% entity coverage
            return False
        
        # Check if denoised text is significantly different from original
        if self._calculate_similarity(denoised_text, original_text) > 0.8:
            return False
        
        return True
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """
        Calculate similarity between two texts.
        
        Args:
            text1: First text
            text2: Second text
            
        Returns:
            Similarity score between 0 and 1
        """
        # Simple character-based similarity
        chars1 = set(text1)
        chars2 = set(text2)
        
        if not chars1 or not chars2:
            return 0.0
        
        intersection = len(chars1.intersection(chars2))
        union = len(chars1.union(chars2))
        
        return intersection / union if union > 0 else 0.0
    
    def _validate_input_pairs(
        self, 
        texts: List[str], 
        entities_list: List[Union[List[str], EntityList]]
    ) -> List[Tuple[str, Union[List[str], EntityList]]]:
        """
        Validate input text-entity pairs.
        
        Args:
            texts: List of texts to validate
            entities_list: List of entity lists or EntityList objects
            
        Returns:
            List of validated (text, entities) tuples
        """
        validated_pairs = []
        
        for i, (text, entities) in enumerate(zip(texts, entities_list)):
            if not text or not text.strip():
                self.logger.log(f"‚ö†Ô∏è Skipping empty text at index {i}")
                continue
            
            # Check text length
            if len(text.strip()) < 10:
                self.logger.log(f"‚ö†Ô∏è Text at index {i} is too short: {len(text.strip())} characters")
                continue
            
            # Validate Chinese text
            if not self.text_processor.is_valid_chinese_text(text):
                self.logger.log(f"‚ö†Ô∏è Text at index {i} may not be valid Chinese text")
                continue
            
            # Validate entities
            entity_strings = self._extract_entity_strings(entities)
            if not entity_strings:
                self.logger.log(f"‚ö†Ô∏è No valid entities found for text at index {i}")
                continue
            
            validated_pairs.append((text.strip(), entities))
        
        if not validated_pairs:
            self.logger.log("‚ùå No valid text-entity pairs found after validation")
        
        return validated_pairs
    
    def _update_statistics(self, original_texts: List[str], denoised_texts: List[str]):
        """
        Update denoising statistics.
        
        Args:
            original_texts: List of original texts
            denoised_texts: List of denoised texts
        """
        self.stats["total_texts_processed"] = len(original_texts)
        self.stats["total_texts_denoised"] = len(denoised_texts)
        self.stats["successful_denoising"] = sum(1 for text in denoised_texts if text)
        self.stats["failed_denoising"] = len(original_texts) - self.stats["successful_denoising"]
        
        # Calculate average compression ratio
        if original_texts and denoised_texts:
            total_original_length = sum(len(text) for text in original_texts)
            total_denoised_length = sum(len(text) for text in denoised_texts if text)
            
            if total_original_length > 0:
                self.stats["average_compression_ratio"] = (
                    total_denoised_length / total_original_length
                )
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        Get denoising statistics.
        
        Returns:
            Dictionary containing denoising statistics
        """
        return self.stats.copy()
    
    def reset_statistics(self):
        """Reset denoising statistics."""
        self.stats = {
            "total_texts_processed": 0,
            "total_texts_denoised": 0,
            "successful_denoising": 0,
            "failed_denoising": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "average_compression_ratio": 0.0
        }
