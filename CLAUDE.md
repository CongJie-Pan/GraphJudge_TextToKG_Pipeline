# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.
- Verify the file status before editing
- If encountering storage issues, please reload the file

## Repository Overview

This is a research codebase for **GraphJudge**, a system that uses LLMs to construct and evaluate knowledge graphs. The project implements a multi-stage pipeline for text-to-KG conversion with LLM-based graph judgment capabilities.

## Key Commands

### Core Pipeline Execution
```bash
# ECTD Stage: Entity extraction and text denoising
python chat/run_entity.py

# Triple Generation Stage: Generate knowledge graph triples
python chat/run_triple.py

# Graph Judge Stage: Evaluate and filter generated triples
python chat/run_gj.py

# Evaluation: Assess final knowledge graph quality
cd graph_evaluation && bash eval.sh
```

### LoRA Fine-tuning for Graph Judgment
```bash
cd graph_judger
python lora_finetune_genwiki_context.py    # For GenWiki dataset
python lora_finetune_rebel_context.py      # For REBEL dataset
python lora_finetune_scierc_context.py     # For SciERC dataset
```

### Model Inference
```bash
cd graph_judger
python lora_infer_batch.py          # Batch inference with fine-tuned model
python lora_infer.py                # Single inference
python lora_infer_batch_naive.py    # Naive baseline inference
```

### Testing
```bash
# Run specific test modules
python -m pytest chat/unit_test/test_*.py
python -m pytest chat/extractEntity_Phase/tests/
python -m pytest chat/graphJudge_Phase/tests/

# Run comprehensive test suites
python chat/unit_test/run_ectd_tests.py
```

## Architecture Overview

### Pipeline Stages (ECTD ‚Üí Triple Generation ‚Üí Graph Judge)

1. **ECTD Stage** (`chat/run_entity.py`, `chat/extractEntity_Phase/`):
   - **Entity Extraction**: Identifies key entities from input text
   - **Text Denoising**: Cleans and restructures text based on extracted entities
   - Uses GPT-5-mini model via LiteLLM for Chinese text processing

2. **Triple Generation Stage** (`chat/run_triple.py`):
   - Generates RDF-style knowledge graph triples from processed text
   - Supports JSON output with schema validation
   - Implements text chunking for large inputs and relation vocabulary standardization

3. **Graph Judge Stage** (`chat/run_gj.py`, `chat/graphJudge_Phase/`):
   - Evaluates correctness of generated triples using LLM judgment
   - Supports multiple APIs (Perplexity, Gemini, KIMI)
   - Includes explainable reasoning and confidence scoring

### Key Directories

- **`chat/`**: Main pipeline scripts and modularized components
  - `run_*.py`: Main execution scripts for each pipeline stage
  - `extractEntity_Phase/`: Modular entity extraction system
  - `graphJudge_Phase/`: Modular graph judgment system
  - `unit_test/`: Comprehensive test suite

- **`graph_judger/`**: LoRA fine-tuning and inference for specialized graph judgment models
  - `lora_finetune_*.py`: Dataset-specific fine-tuning scripts
  - `lora_infer*.py`: Inference scripts for trained models

- **`graph_evaluation/`**: Evaluation metrics and benchmarking
  - `metrics/eval.py`: Core evaluation logic
  - `eval.sh`: Evaluation runner script

- **`datasets/`**: Training data and results storage
- **`config/`**: Pipeline configuration files
- **`tools/`**: Utility scripts for data processing

### Configuration System

The pipeline uses YAML-based configuration (`config/pipeline_config.yaml`) with:
- Stage-specific settings for ECTD, triple generation, and graph judgment
- API rate limiting and error handling configuration
- Quality assurance and monitoring settings

### Data Flow

1. **Input**: Raw text (supports Chinese classical literature like Á¥ÖÊ®ìÂ§¢)
2. **ECTD Processing**: Entities extracted ‚Üí Text denoised
3. **Triple Generation**: Structured knowledge graph triples in JSON format
4. **Graph Judgment**: LLM-based triple validation with reasoning
5. **Output**: Filtered, high-quality knowledge graph

## Development Notes

- The codebase primarily uses Python with async processing patterns
- Supports multiple LLM APIs through LiteLLM integration
- Implements comprehensive caching and rate limiting
- Uses Pydantic for data validation and PyTest for testing
- Configuration is centralized in YAML files with environment-specific overrides
- Results are stored in timestamped iteration directories under `datasets/`

## Working with Tests

- Test files follow the pattern `test_*.py` and are distributed across component directories
- Use `python -m pytest` for running tests with proper module resolution
- Some tests require API keys to be configured in environment variable

## Coding Guidelines

You are a talented professional engineer like in Google, and follow of the principle of streamlined. Please adhere to the following guidelines:

## üö® CRITICAL RULES - READ FIRST

> **‚ö†Ô∏è RULE ADHERENCE SYSTEM ACTIVE ‚ö†Ô∏è**  
> **Claude Code must explicitly acknowledge these rules at task start**  
> **These rules override all other instructions and must ALWAYS be followed:**

### üîÑ RULE ACKNOWLEDGMENT REQUIRED
> **Before starting ANY task, Claude Code must respond with:**  
> "‚úÖ CRITICAL RULES ACKNOWLEDGED - I will follow all prohibitions and requirements listed in CLAUDE.md"

### ‚ùå ABSOLUTE PROHIBITIONS
- **NEVER** create new files in root directory ‚Üí use proper module structure
- **NEVER** write output files directly to root directory ‚Üí use designated output folders
- **NEVER** create documentation files (.md) unless explicitly requested by user
- **NEVER** use `find`, `grep`, `cat`, `head`, `tail`, `ls` commands ‚Üí use Read, LS, Grep, Glob tools instead
- **NEVER** create duplicate files (manager_v2.py, enhanced_xyz.py, utils_new.js) ‚Üí ALWAYS extend existing files
- **NEVER** create multiple implementations of same concept ‚Üí single source of truth
- **NEVER** copy-paste code blocks ‚Üí extract into shared utilities/functions
- **NEVER** hardcode values that should be configurable ‚Üí use config files/environment variables
- **NEVER** use naming like enhanced_, improved_, new_, v2_ ‚Üí extend original files instead
- **NEVER** assume missing context ‚Üí ask questions if uncertain
- **NEVER** hallucinate libraries or functions ‚Üí only use known, verified packages
- **NEVER** delete or overwrite existing code unless explicitly instructed

### üìù MANDATORY REQUIREMENTS
- **COMMIT** after every completed task/phase - no exceptions
- **USE TASK AGENTS** for all long-running operations (>30 seconds) - Bash commands stop when context switches
- **TODOWRITE** for complex tasks (3+ steps) ‚Üí parallel agents ‚Üí git checkpoints ‚Üí test validation
- **READ FILES FIRST** before editing - Edit/Write tools will fail if you didn't read the file first
- **DEBT PREVENTION** - Before creating new files, check for existing similar functionality to extend  
- **SINGLE SOURCE OF TRUTH** - One authoritative implementation per feature/concept
- **TEST FIRST DEVELOPMENT** - Write tests before implementation to ensure interface consistency
- **FILE SIZE LIMIT** - Never create files longer than 500 lines of code ‚Üí refactor by splitting into modules

## üèóÔ∏è CODING GUIDELINES & STANDARDS

### üó®Ô∏è Communication & Language Standards
- All code‚Äîincluding comments, variable names, function names, logs, and documentation‚Äîmust be written in English
- Provide detailed comments for each code block, explaining the purpose and logic of every function, class, and significant code section
- Include thorough explanations of why specific approaches were chosen
- All error logging, debugging information, and technical documentation should be in English
- Ensure code is well-structured, follows best practices, and includes comprehensive error handling with clear English error messages

### üîÑ Project Awareness & Context Management
- Always read `PLANNING.md` at the start of a new conversation to understand the project's architecture, goals, style, and constraints
- Check `TASK.md` before starting a new task. If the task isn't listed, add it with a brief description and today's date
- Use consistent naming conventions, file structure, and architecture patterns as described in `PLANNING.md`

### üß± Code Structure & Modularity Principles
- Never create a file longer than 500 lines of code. If a file approaches this limit, refactor by splitting it into modules or helper files
- Organize code into clearly separated modules, grouped by feature or responsibility
- Use clear, consistent imports (prefer relative imports within packages)

### üìé Language-Specific Style & Conventions
- **Python Projects**: Follow PEP8, use type hints, format with `black`
- **JavaScript/TypeScript**: Follow ESLint standards, use TypeScript when possible
- **Java**: Follow Oracle coding conventions, use Maven/Gradle structure
- Use `pydantic` for data validation in Python projects
- Use `FastAPI` for APIs and `SQLAlchemy` or `SQLModel` for ORM if applicable
- Write docstrings for every function using the Google style:
  ```
  def example():
      """
      Brief summary.

      Args:
          param1 (type): Description.

      Returns:
          type: Description.
      """
  ```

## üß™ COMPREHENSIVE TESTING FRAMEWORK

### üéØ Test-Driven Development (TDD) Workflow
- **Write tests first, implement later** - Follow TDD principles to ensure interface design consistency
- Define expected behavior through tests before writing implementation code
- Use tests as living documentation of system requirements and expected behavior
- **Ensure alignment between tests and implementation** - Tests must use the same calling patterns as the actual implementation
- If tests expect `object.attribute.method()`, implementation must provide that exact interface
- If implementation uses static methods `Class.method()`, tests should mirror this pattern

### üìä Test Coverage & Quality Requirements
- **Always create unit tests for new features** (functions, classes, routes, etc)
- **Update existing tests when logic changes** - After any logic modification, verify and update related tests
- Tests should live in a `/tests` folder mirroring the main application structure
- Each feature must include at least:
  - **1 test for expected use case** - Normal, successful operation
  - **1 edge case test** - Boundary conditions, unusual but valid inputs
  - **1 failure case test** - Invalid inputs, error conditions, exception handling

### üé≠ Mock and Test Design Best Practices
- **Use module-level variables for aligned test mock paths** - Ensure mocks target the correct import paths
- **Access and call objects meaningfully** - Don't just import to confirm existence; actually invoke methods to match mock design
- **Trigger actual mock behavior** - Use real calls/await statements to activate mock responses
- **Design for testability** - Structure code to allow easy mocking and testing
- **Cross-platform consistency** - Don't rely on OS-specific path interpretation
- Ensure tests behave identically across different operating systems

### üõ°Ô∏è Error Handling & Performance Testing
- **Comprehensive Error Testing** - Test error propagation across module boundaries
- Verify graceful handling of file system errors
- Test configuration error scenarios (missing API keys, invalid settings)
- Ensure error messages are clear and actionable
- **Performance Testing** - Test batch processing performance with realistic data sizes
- Verify memory usage stability during extended operations
- Test proper cleanup of resources (files, connections, etc.)

### üìö Test Documentation & Maintenance Standards
- Use clear, descriptive test method names
- Include docstrings explaining test purpose and expected behavior
- Document any special setup or teardown requirements
- Maintain test documentation alongside code changes
- Regular review of test relevance and effectiveness
- Remove or update obsolete tests
- Monitor test flakiness and address unstable tests promptly

### ‚úÖ Quality Assurance Checklist
Before considering any feature complete:
- [ ] All new functionality has corresponding tests
- [ ] Tests cover normal, edge, and failure cases
- [ ] Test-implementation architectural consistency verified
- [ ] Cross-platform compatibility confirmed
- [ ] Error handling thoroughly tested
- [ ] Performance impact assessed
- [ ] Documentation updated accordingly
- [ ] Code review completed with focus on test quality

## üèóÔ∏è PROJECT STRUCTURE TEMPLATES

### üìÅ Simple Project Structure
```
project-root/
‚îú‚îÄ‚îÄ CLAUDE.md              # Essential rules for Claude Code
‚îú‚îÄ‚îÄ README.md              # Project documentation
‚îú‚îÄ‚îÄ .gitignore             # Git ignore patterns
‚îú‚îÄ‚îÄ src/                   # Source code (NEVER put files in root)
‚îÇ   ‚îú‚îÄ‚îÄ main.py            # Main script/entry point
‚îÇ   ‚îî‚îÄ‚îÄ utils.py           # Utility functions
‚îú‚îÄ‚îÄ tests/                 # Test files
‚îÇ   ‚îî‚îÄ‚îÄ test_main.py       # Basic tests
‚îú‚îÄ‚îÄ docs/                  # Documentation
‚îî‚îÄ‚îÄ output/                # Generated output files
```

### üìÅ Standard Project Structure
```
project-root/
‚îú‚îÄ‚îÄ CLAUDE.md              # Essential rules for Claude Code
‚îú‚îÄ‚îÄ README.md              # Project documentation
‚îú‚îÄ‚îÄ LICENSE                # Project license
‚îú‚îÄ‚îÄ .gitignore             # Git ignore patterns
‚îú‚îÄ‚îÄ src/                   # Source code (NEVER put files in root)
‚îÇ   ‚îú‚îÄ‚îÄ main/              # Main application code
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ [language]/    # Language-specific code
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/      # Core business logic
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/     # Utility functions/classes
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/    # Data models/entities
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/  # Service layer
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api/       # API endpoints/interfaces
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ resources/     # Non-code resources
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ config/    # Configuration files
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ assets/    # Static assets
‚îÇ   ‚îî‚îÄ‚îÄ test/              # Test code
‚îÇ       ‚îú‚îÄ‚îÄ unit/          # Unit tests
‚îÇ       ‚îî‚îÄ‚îÄ integration/   # Integration tests
‚îú‚îÄ‚îÄ docs/                  # Documentation
‚îú‚îÄ‚îÄ tools/                 # Development tools and scripts
‚îú‚îÄ‚îÄ examples/              # Usage examples
‚îî‚îÄ‚îÄ output/                # Generated output files
```

### üìÅ AI/ML Project Structure
```
project-root/
‚îú‚îÄ‚îÄ CLAUDE.md              # Essential rules for Claude Code
‚îú‚îÄ‚îÄ README.md              # Project documentation
‚îú‚îÄ‚îÄ LICENSE                # Project license
‚îú‚îÄ‚îÄ .gitignore             # Git ignore patterns
‚îú‚îÄ‚îÄ src/                   # Source code (NEVER put files in root)
‚îÇ   ‚îú‚îÄ‚îÄ main/              # Main application code
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ python/        # Python-specific code
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/      # Core ML algorithms
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/     # Data processing utilities
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/    # Model definitions/architectures
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/  # ML services and pipelines
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/       # ML API endpoints/interfaces
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training/  # Training scripts and pipelines
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inference/ # Inference and prediction code
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluation/# Model evaluation and metrics
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ resources/     # Non-code resources
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ config/    # Configuration files
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ data/      # Sample/seed data
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ assets/    # Static assets
‚îÇ   ‚îî‚îÄ‚îÄ test/              # Test code
‚îÇ       ‚îú‚îÄ‚îÄ unit/          # Unit tests
‚îÇ       ‚îú‚îÄ‚îÄ integration/   # Integration tests
‚îÇ       ‚îî‚îÄ‚îÄ fixtures/      # Test data/fixtures
‚îú‚îÄ‚îÄ data/                  # AI/ML Dataset management
‚îÇ   ‚îú‚îÄ‚îÄ raw/               # Original, unprocessed datasets
‚îÇ   ‚îú‚îÄ‚îÄ processed/         # Cleaned and transformed data
‚îÇ   ‚îú‚îÄ‚îÄ external/          # External data sources
‚îÇ   ‚îî‚îÄ‚îÄ temp/              # Temporary data processing files
‚îú‚îÄ‚îÄ notebooks/             # Jupyter notebooks and analysis
‚îÇ   ‚îú‚îÄ‚îÄ exploratory/       # Data exploration notebooks
‚îÇ   ‚îú‚îÄ‚îÄ experiments/       # ML experiments and prototyping
‚îÇ   ‚îî‚îÄ‚îÄ reports/           # Analysis reports and visualizations
‚îú‚îÄ‚îÄ models/                # ML Models and artifacts
‚îÇ   ‚îú‚îÄ‚îÄ trained/           # Trained model files
‚îÇ   ‚îú‚îÄ‚îÄ checkpoints/       # Model checkpoints
‚îÇ   ‚îî‚îÄ‚îÄ metadata/          # Model metadata and configs
‚îú‚îÄ‚îÄ experiments/           # ML Experiment tracking
‚îÇ   ‚îú‚îÄ‚îÄ configs/           # Experiment configurations
‚îÇ   ‚îú‚îÄ‚îÄ results/           # Experiment results and metrics
‚îÇ   ‚îî‚îÄ‚îÄ logs/              # Training logs and metrics
‚îú‚îÄ‚îÄ docs/                  # Documentation
‚îú‚îÄ‚îÄ tools/                 # Development tools and scripts
‚îú‚îÄ‚îÄ examples/              # Usage examples
‚îî‚îÄ‚îÄ output/                # Generated output files
```

## ‚úÖ TASK COMPLETION & DOCUMENTATION STANDARDS

### üìù Task Management Requirements
- Mark completed tasks in `TASK.md` immediately after finishing them
- Add new sub-tasks or TODOs discovered during development to `TASK.md` under a "Discovered During Work" section

### üìö Documentation & Explainability Standards
- Update `README.md` when new features are added, dependencies change, or setup steps are modified
- Comment non-obvious code and ensure everything is understandable to a mid-level developer
- When writing complex logic, add an inline `# Reason:` comment explaining the why, not just the what

## üö® TECHNICAL DEBT PREVENTION SYSTEM

### üîç MANDATORY PRE-TASK COMPLIANCE CHECK
> **STOP: Before starting any task, Claude Code must explicitly verify ALL points:**

**Step 1: Rule Acknowledgment**
- [ ] ‚úÖ I acknowledge all critical rules in CLAUDE.md and will follow them

**Step 2: Task Analysis**  
- [ ] Will this create files in root? ‚Üí If YES, use proper module structure instead
- [ ] Will this take >30 seconds? ‚Üí If YES, use Task agents not Bash
- [ ] Is this 3+ steps? ‚Üí If YES, use TodoWrite breakdown first
- [ ] Am I about to use grep/find/cat? ‚Üí If YES, use proper tools instead

**Step 3: Technical Debt Prevention (MANDATORY SEARCH FIRST)**
- [ ] **SEARCH FIRST**: Use Grep pattern="<functionality>.*<keyword>" to find existing implementations
- [ ] **CHECK EXISTING**: Read any found files to understand current functionality
- [ ] Does similar functionality already exist? ‚Üí If YES, extend existing code
- [ ] Am I creating a duplicate class/manager? ‚Üí If YES, consolidate instead
- [ ] Will this create multiple sources of truth? ‚Üí If YES, redesign approach
- [ ] Have I searched for existing implementations? ‚Üí Use Grep/Glob tools first
- [ ] Can I extend existing code instead of creating new? ‚Üí Prefer extension over creation
- [ ] Am I about to copy-paste code? ‚Üí Extract to shared utility instead

**Step 4: Session Management**
- [ ] Is this a long/complex task? ‚Üí If YES, plan context checkpoints
- [ ] Have I been working >1 hour? ‚Üí If YES, consider /compact or session break

> **‚ö†Ô∏è DO NOT PROCEED until all checkboxes are explicitly verified**

### üßπ Debt Prevention Workflow Pattern
**Before Creating ANY New File:**
1. **üîç Search First** - Use Grep/Glob to find existing implementations
2. **üìã Analyze Existing** - Read and understand current patterns
3. **ü§î Decision Tree**: Can extend existing? ‚Üí DO IT | Must create new? ‚Üí Document why
4. **‚úÖ Follow Patterns** - Use established project patterns
5. **üìà Validate** - Ensure no duplication or technical debt

**Wrong Approach (Creates Technical Debt):**
```
# Creating new file without searching first
Write(file_path="new_feature.py", content="...")
```

**Correct Approach (Prevents Technical Debt):**
```
# 1. SEARCH FIRST
Grep(pattern="feature.*implementation", include="*.py")
# 2. READ EXISTING FILES  
Read(file_path="existing_feature.py")
# 3. EXTEND EXISTING FUNCTIONALITY
Edit(file_path="existing_feature.py", old_string="...", new_string="...")
```

**‚ö†Ô∏è Prevention is better than consolidation - build clean from the start.**  
**üéØ Focus on single source of truth and extending existing functionality.**  
**üìà Each task should maintain clean architecture and prevent technical debt.**  
**üß™ Remember: Tests first, implementation second - this ensures better design.**

> **‚ö†Ô∏è DO NOT PROCEED until all checkboxes are explicitly verified**