"""
Result Display Functions for GraphJudge Streamlit Application.

This module provides specialized display functions for different types of
pipeline results and data visualizations. It complements the components
module with focused display logic for complex data structures.
"""

import streamlit as st
import pandas as pd
from typing import List, Dict, Any, Optional
import json
from datetime import datetime

# Optional plotly import with graceful fallback
try:
    import plotly.express as px
    import plotly.graph_objects as go
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False
    px = None
    go = None

from ..core.models import EntityResult, TripleResult, JudgmentResult, Triple
from ..core.pipeline import PipelineResult


def display_final_results(pipeline_result: PipelineResult):
    """
    Display the final consolidated results from all pipeline stages.
    
    Args:
        pipeline_result: Complete pipeline execution results
    """
    st.markdown("# ğŸ† æœ€ç»ˆç»“æœ (Final Results)")
    
    if not pipeline_result.success:
        st.error(f"Pipeline failed at {pipeline_result.error_stage}: {pipeline_result.error}")
        return
    
    # Get the approved triples
    if (pipeline_result.judgment_result and 
        pipeline_result.triple_result and 
        pipeline_result.judgment_result.judgments):
        
        approved_triples = [
            triple for triple, approved in zip(
                pipeline_result.triple_result.triples,
                pipeline_result.judgment_result.judgments
            ) if approved
        ]
        
        rejected_triples = [
            triple for triple, approved in zip(
                pipeline_result.triple_result.triples,
                pipeline_result.judgment_result.judgments
            ) if not approved
        ]
        
        # Summary metrics
        total_triples = len(pipeline_result.triple_result.triples)
        approved_count = len(approved_triples)
        rejection_rate = (total_triples - approved_count) / total_triples if total_triples > 0 else 0
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "âœ… é€šè¿‡çš„ä¸‰å…ƒç»„",
                approved_count,
                delta=f"{(1-rejection_rate)*100:.1f}% é€šè¿‡ç‡"
            )
        
        with col2:
            st.metric(
                "âŒ è¢«æ‹’ç»çš„",
                total_triples - approved_count,
                delta=f"{rejection_rate*100:.1f}% æ‹’ç»ç‡"
            )
        
        with col3:
            avg_confidence = (
                sum(pipeline_result.judgment_result.confidence) / 
                len(pipeline_result.judgment_result.confidence)
                if pipeline_result.judgment_result.confidence else 0
            )
            st.metric(
                "ğŸ¯ å¹³å‡ç½®ä¿¡åº¦",
                f"{avg_confidence:.3f}"
            )
        
        with col4:
            st.metric(
                "â±ï¸ æ€»å¤„ç†æ—¶é—´",
                f"{pipeline_result.total_time:.1f}s"
            )
        
        # Display approved triples as the final knowledge graph
        if approved_triples:
            st.markdown("## ğŸ§  æœ€ç»ˆçŸ¥è¯†å›¾è°±")
            st.markdown(f"ç»è¿‡AIåˆ¤æ–­åï¼Œä»¥ä¸‹ **{len(approved_triples)}** ä¸ªçŸ¥è¯†ä¸‰å…ƒç»„è¢«è®¤ä¸ºæ˜¯å‡†ç¡®çš„ï¼š")
            
            display_final_knowledge_graph(approved_triples, pipeline_result.judgment_result)
            
            # Export options
            st.markdown("### ğŸ“¤ å¯¼å‡ºé€‰é¡¹")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("ğŸ“„ å¯¼å‡ºä¸º JSON", key="export_final_json"):
                    export_final_results_json(approved_triples, pipeline_result)
            
            with col2:
                if st.button("ğŸ“Š å¯¼å‡ºä¸º CSV", key="export_final_csv"):
                    export_final_results_csv(approved_triples, pipeline_result)
            
            with col3:
                if st.button("ğŸ“‹ ç”ŸæˆæŠ¥å‘Š", key="generate_report"):
                    display_analysis_report(pipeline_result)
        
        else:
            st.warning("âš ï¸ æ²¡æœ‰ä¸‰å…ƒç»„é€šè¿‡AIåˆ¤æ–­ã€‚æ‚¨å¯èƒ½éœ€è¦è°ƒæ•´è¾“å…¥æ–‡æœ¬æˆ–æ£€æŸ¥å¤„ç†é€»è¾‘ã€‚")
            
            # Show rejected triples for reference
            if rejected_triples:
                with st.expander("ğŸ” æŸ¥çœ‹è¢«æ‹’ç»çš„ä¸‰å…ƒç»„"):
                    display_rejected_triples_analysis(rejected_triples, pipeline_result.judgment_result)


def display_final_knowledge_graph(triples: List[Triple], judgment_result: JudgmentResult):
    """
    Display the final approved knowledge graph in an attractive format.
    
    Args:
        triples: List of approved triples
        judgment_result: Judgment results for confidence scores
    """
    # Create a beautiful table format
    final_data = []
    for i, triple in enumerate(triples):
        confidence_idx = None
        # Find the original index of this triple for confidence
        if judgment_result.confidence:
            confidence_idx = i
        
        confidence = (judgment_result.confidence[confidence_idx] 
                     if confidence_idx is not None and confidence_idx < len(judgment_result.confidence)
                     else 0.0)
        
        # Create a formatted entry
        final_data.append({
            "åºå·": i + 1,
            "çŸ¥è¯†ä¸‰å…ƒç»„": f"ã€{triple.subject}ã€‘ â†’ {triple.predicate} â†’ ã€{triple.object}ã€‘",
            "ä¸»è¯­": triple.subject,
            "å…³ç³»": triple.predicate,
            "å®¾è¯­": triple.object,
            "AIç½®ä¿¡åº¦": f"{confidence:.3f}" if confidence > 0 else "N/A",
            "è´¨é‡ç­‰çº§": get_quality_grade(confidence) if confidence > 0 else "æœªè¯„çº§"
        })
    
    df = pd.DataFrame(final_data)
    
    # Display with custom styling
    st.markdown("### ğŸ“‹ çŸ¥è¯†ä¸‰å…ƒç»„è¯¦æƒ…")
    
    # Interactive data table with selection
    selected_indices = st.dataframe(
        df[["åºå·", "çŸ¥è¯†ä¸‰å…ƒç»„", "AIç½®ä¿¡åº¦", "è´¨é‡ç­‰çº§"]],
        use_container_width=True,
        hide_index=True,
        on_select="rerun",
        selection_mode="multi-row",
        column_config={
            "çŸ¥è¯†ä¸‰å…ƒç»„": st.column_config.TextColumn(
                "çŸ¥è¯†ä¸‰å…ƒç»„",
                help="ç‚¹å‡»æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯",
                width="large"
            ),
            "AIç½®ä¿¡åº¦": st.column_config.ProgressColumn(
                "AIç½®ä¿¡åº¦",
                min_value=0.0,
                max_value=1.0,
                format="%.3f"
            ),
            "è´¨é‡ç­‰çº§": st.column_config.TextColumn(
                "è´¨é‡ç­‰çº§",
                help="åŸºäºç½®ä¿¡åº¦çš„è´¨é‡è¯„çº§"
            )
        }
    )
    
    # Show knowledge graph visualization
    if len(triples) > 1:
        st.markdown("### ğŸ•¸ï¸ å…³ç³»ç½‘ç»œå›¾")
        if PLOTLY_AVAILABLE:
            create_enhanced_knowledge_graph(triples)
        else:
            st.info("ğŸ“Š ç½‘ç»œå›¾éœ€è¦å®‰è£… Plotly åº“: `pip install plotly`")
            st.text("æ–‡æœ¬å½¢å¼çš„å…³ç³»å±•ç¤º:")
            for i, triple in enumerate(triples[:15], 1):
                st.text(f"{i}. {triple.subject} â†’ {triple.predicate} â†’ {triple.object}")


def display_rejected_triples_analysis(rejected_triples: List[Triple], judgment_result: JudgmentResult):
    """
    Display analysis of rejected triples to help users understand the filtering.
    
    Args:
        rejected_triples: List of rejected triples
        judgment_result: Judgment results with explanations
    """
    st.markdown("#### è¢«æ‹’ç»çš„ä¸‰å…ƒç»„åˆ†æ")
    
    rejection_data = []
    explanation_idx = 0
    
    for triple in rejected_triples:
        # Find explanation if available
        explanation = None
        if (judgment_result.explanations and 
            explanation_idx < len(judgment_result.explanations)):
            explanation = judgment_result.explanations[explanation_idx]
        
        rejection_data.append({
            "ä¸‰å…ƒç»„": f"{triple.subject} - {triple.predicate} - {triple.object}",
            "å¯èƒ½åŸå› ": explanation or "AIåˆ¤æ–­è¯¥å…³ç³»ä¸å¤Ÿå‡†ç¡®æˆ–ç›¸å…³",
            "å»ºè®®": get_rejection_suggestion(triple, explanation)
        })
        explanation_idx += 1
    
    df = pd.DataFrame(rejection_data)
    st.dataframe(df, use_container_width=True, hide_index=True)


def create_enhanced_knowledge_graph(triples: List[Triple]):
    """
    Create an enhanced interactive knowledge graph visualization.
    
    Args:
        triples: List of triples to visualize
    """
    if not PLOTLY_AVAILABLE:
        st.error("Plotlyåº“æœªå®‰è£…ï¼Œæ— æ³•æ˜¾ç¤ºå›¾å½¢å¯è§†åŒ–")
        return
    
    try:
        # Extract entities and relationships
        entities = set()
        relationships = []
        
        for triple in triples:
            entities.add(triple.subject)
            entities.add(triple.object)
            relationships.append({
                'source': triple.subject,
                'target': triple.object,
                'relation': triple.predicate,
                'confidence': triple.confidence or 0.5
            })
        
        entities = list(entities)
        
        # Limit visualization size for performance
        if len(entities) > 15:
            st.warning(f"å®ä½“æ•°é‡è¾ƒå¤š({len(entities)}ä¸ª)ï¼Œæ˜¾ç¤ºå‰15ä¸ªå®ä½“çš„å…³ç³»å›¾")
            entities = entities[:15]
            relationships = [r for r in relationships 
                           if r['source'] in entities and r['target'] in entities]
        
        # Create network graph using Plotly
        # Position entities using a simple circular layout
        import math
        positions = {}
        n = len(entities)
        
        for i, entity in enumerate(entities):
            angle = 2 * math.pi * i / n
            radius = 3
            positions[entity] = {
                'x': radius * math.cos(angle),
                'y': radius * math.sin(angle)
            }
        
        # Create the visualization
        fig = go.Figure()
        
        # Add edges
        for rel in relationships:
            source_pos = positions.get(rel['source'])
            target_pos = positions.get(rel['target'])
            
            if source_pos and target_pos:
                # Draw edge
                fig.add_trace(go.Scatter(
                    x=[source_pos['x'], target_pos['x'], None],
                    y=[source_pos['y'], target_pos['y'], None],
                    mode='lines',
                    line=dict(
                        width=2 + rel['confidence'] * 3,  # Thickness based on confidence
                        color=f"rgba(100, 100, 100, {0.3 + rel['confidence'] * 0.7})"
                    ),
                    hoverinfo='none',
                    showlegend=False
                ))
                
                # Add relationship label
                mid_x = (source_pos['x'] + target_pos['x']) / 2
                mid_y = (source_pos['y'] + target_pos['y']) / 2
                
                fig.add_annotation(
                    x=mid_x,
                    y=mid_y,
                    text=rel['relation'],
                    showarrow=False,
                    font=dict(size=10, color='blue'),
                    bgcolor="rgba(255,255,255,0.8)",
                    bordercolor="blue",
                    borderwidth=1,
                    borderpad=2
                )
        
        # Add nodes
        for entity in entities:
            pos = positions[entity]
            fig.add_trace(go.Scatter(
                x=[pos['x']],
                y=[pos['y']],
                mode='markers+text',
                text=[entity],
                textposition='middle center',
                textfont=dict(size=10, color='white'),
                marker=dict(
                    size=40,
                    color='lightblue',
                    line=dict(width=3, color='darkblue')
                ),
                hoverinfo='text',
                hovertext=entity,
                showlegend=False
            ))
        
        # Update layout
        fig.update_layout(
            title={
                'text': "çŸ¥è¯†å›¾è°±ç½‘ç»œå¯è§†åŒ–",
                'x': 0.5,
                'font': {'size': 16}
            },
            showlegend=False,
            hovermode='closest',
            margin=dict(b=20, l=5, r=5, t=40),
            annotations=[dict(
                text="èŠ‚ç‚¹ï¼šå®ä½“ | è¾¹ï¼šå…³ç³» | çº¿æ¡ç²—ç»†ï¼šAIç½®ä¿¡åº¦",
                showarrow=False,
                xref="paper", yref="paper",
                x=0.005, y=-0.002,
                xanchor='left', yanchor='bottom',
                font=dict(size=12, color='gray')
            )],
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            plot_bgcolor='white',
            height=500
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
    except Exception as e:
        st.error(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {str(e)}")
        st.info("æ‚¨ä»å¯ä»¥æŸ¥çœ‹ä¸Šæ–¹çš„è¡¨æ ¼å½¢å¼ç»“æœ")


def export_final_results_json(triples: List[Triple], pipeline_result: PipelineResult):
    """Export final results as JSON format."""
    export_data = {
        "metadata": {
            "export_time": datetime.now().isoformat(),
            "total_processing_time": pipeline_result.total_time,
            "pipeline_success": pipeline_result.success,
            "total_triples_generated": len(pipeline_result.triple_result.triples) if pipeline_result.triple_result else 0,
            "approved_triples_count": len(triples),
            "approval_rate": len(triples) / len(pipeline_result.triple_result.triples) if pipeline_result.triple_result and pipeline_result.triple_result.triples else 0
        },
        "knowledge_graph": [
            {
                "subject": triple.subject,
                "predicate": triple.predicate,
                "object": triple.object,
                "confidence": triple.confidence or 0.0
            }
            for triple in triples
        ],
        "processing_stats": pipeline_result.stats or {}
    }
    
    json_str = json.dumps(export_data, ensure_ascii=False, indent=2)
    
    st.download_button(
        label="ğŸ“ ä¸‹è½½ JSON æ–‡ä»¶",
        data=json_str,
        file_name=f"knowledge_graph_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json"
    )


def export_final_results_csv(triples: List[Triple], pipeline_result: PipelineResult):
    """Export final results as CSV format."""
    csv_data = []
    
    for i, triple in enumerate(triples):
        csv_data.append({
            "åºå·": i + 1,
            "ä¸»è¯­": triple.subject,
            "è°“è¯­": triple.predicate,
            "å®¾è¯­": triple.object,
            "ç½®ä¿¡åº¦": triple.confidence or 0.0,
            "è´¨é‡ç­‰çº§": get_quality_grade(triple.confidence or 0.0)
        })
    
    df = pd.DataFrame(csv_data)
    csv_string = df.to_csv(index=False, encoding='utf-8')
    
    st.download_button(
        label="ğŸ“Š ä¸‹è½½ CSV æ–‡ä»¶", 
        data=csv_string,
        file_name=f"knowledge_graph_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        mime="text/csv"
    )


def display_analysis_report(pipeline_result: PipelineResult):
    """Display a comprehensive analysis report."""
    st.markdown("## ğŸ“Š åˆ†ææŠ¥å‘Š")
    
    # Generate report content
    report_sections = []
    
    # Executive Summary
    if pipeline_result.success:
        approved_count = sum(pipeline_result.judgment_result.judgments) if pipeline_result.judgment_result else 0
        total_count = len(pipeline_result.triple_result.triples) if pipeline_result.triple_result else 0
        approval_rate = approved_count / total_count if total_count > 0 else 0
        
        report_sections.append(f"""
        ### ğŸ“‹ æ‰§è¡Œæ‘˜è¦
        
        - **æ€»ä½“çŠ¶æ€**: âœ… æˆåŠŸå®Œæˆ
        - **å¤„ç†æ—¶é—´**: {pipeline_result.total_time:.2f} ç§’
        - **çŸ¥è¯†æå–**: ä»è¾“å…¥æ–‡æœ¬ä¸­æˆåŠŸæå–äº† {approved_count} ä¸ªé«˜è´¨é‡çŸ¥è¯†ä¸‰å…ƒç»„
        - **è´¨é‡è¯„çº§**: {approval_rate:.1%} çš„ç”Ÿæˆä¸‰å…ƒç»„é€šè¿‡äº†AIè´¨é‡æ£€æŸ¥
        """)
    
    # Stage Analysis
    if pipeline_result.entity_result:
        entities_count = len(pipeline_result.entity_result.entities)
        report_sections.append(f"""
        ### ğŸ” å®ä½“æå–åˆ†æ
        
        - **å®ä½“æ•°é‡**: {entities_count} ä¸ª
        - **å¤„ç†æ—¶é—´**: {pipeline_result.entity_result.processing_time:.2f} ç§’
        - **æ•ˆç‡**: {entities_count/pipeline_result.entity_result.processing_time:.1f} å®ä½“/ç§’
        """)
    
    if pipeline_result.triple_result:
        triples_count = len(pipeline_result.triple_result.triples)
        report_sections.append(f"""
        ### ğŸ”— ä¸‰å…ƒç»„ç”Ÿæˆåˆ†æ
        
        - **ç”Ÿæˆæ•°é‡**: {triples_count} ä¸ªä¸‰å…ƒç»„
        - **å¤„ç†æ—¶é—´**: {pipeline_result.triple_result.processing_time:.2f} ç§’
        - **ç”Ÿæˆæ•ˆç‡**: {triples_count/pipeline_result.triple_result.processing_time:.1f} ä¸‰å…ƒç»„/ç§’
        """)
    
    # Quality Analysis
    if pipeline_result.judgment_result:
        high_quality = sum(1 for c in pipeline_result.judgment_result.confidence if c > 0.8)
        medium_quality = sum(1 for c in pipeline_result.judgment_result.confidence if 0.5 <= c <= 0.8)
        low_quality = sum(1 for c in pipeline_result.judgment_result.confidence if c < 0.5)
        
        report_sections.append(f"""
        ### âš–ï¸ è´¨é‡åˆ†æ
        
        - **é«˜è´¨é‡** (>0.8): {high_quality} ä¸ª
        - **ä¸­ç­‰è´¨é‡** (0.5-0.8): {medium_quality} ä¸ª
        - **å¾…æ”¹è¿›** (<0.5): {low_quality} ä¸ª
        - **å¹³å‡ç½®ä¿¡åº¦**: {sum(pipeline_result.judgment_result.confidence)/len(pipeline_result.judgment_result.confidence):.3f}
        """)
    
    # Display report
    for section in report_sections:
        st.markdown(section)
    
    # Recommendations
    st.markdown("""
    ### ğŸ’¡ å»ºè®®
    
    1. **é«˜è´¨é‡ç»“æœ**: ç½®ä¿¡åº¦è¶…è¿‡0.8çš„ä¸‰å…ƒç»„å¯ä»¥ç›´æ¥ä½¿ç”¨
    2. **äººå·¥å®¡æ ¸**: å»ºè®®å¯¹ç½®ä¿¡åº¦0.5-0.8çš„ç»“æœè¿›è¡Œäººå·¥æ£€æŸ¥
    3. **ç»“æœä¼˜åŒ–**: å¦‚éœ€æ›´å¤šé«˜è´¨é‡ç»“æœï¼Œå¯å°è¯•è°ƒæ•´è¾“å…¥æ–‡æœ¬çš„è¡¨è¿°æ–¹å¼
    """)


def get_quality_grade(confidence: float) -> str:
    """Convert confidence score to quality grade."""
    if confidence >= 0.9:
        return "ğŸ† ä¼˜ç§€"
    elif confidence >= 0.8:
        return "ğŸ¥‡ è‰¯å¥½"
    elif confidence >= 0.6:
        return "ğŸ¥ˆ ä¸­ç­‰"
    elif confidence >= 0.4:
        return "ğŸ¥‰ ä¸€èˆ¬"
    else:
        return "âš ï¸ å¾…æ”¹è¿›"


def get_rejection_suggestion(triple: Triple, explanation: Optional[str]) -> str:
    """Generate suggestion for rejected triples."""
    if explanation and "ä¸å‡†ç¡®" in explanation:
        return "æ£€æŸ¥ä¸»è¯­å’Œå®¾è¯­çš„å…³ç³»æ˜¯å¦æ­£ç¡®è¡¨è¿°"
    elif explanation and "ä¸ç›¸å…³" in explanation:
        return "ç¡®è®¤è¯¥å…³ç³»æ˜¯å¦ä¸ä¸»é¢˜ç›¸å…³"
    elif explanation and "æ¨¡ç³Š" in explanation:
        return "å°è¯•ä½¿ç”¨æ›´æ˜ç¡®çš„è¡¨è¿°"
    else:
        return "é‡æ–°å®¡è§†è¯¥å…³ç³»çš„è¡¨è¿°æ–¹å¼æˆ–ä¸Šä¸‹æ–‡"


def display_comparison_view(current_result: PipelineResult, previous_results: List[PipelineResult]):
    """
    Display comparison between current and previous results.
    
    Args:
        current_result: Current pipeline result
        previous_results: List of previous results for comparison
    """
    if not previous_results:
        return
    
    st.markdown("## ğŸ“ˆ å†å²å¯¹æ¯”")
    
    # Create comparison metrics
    comparison_data = []
    for i, result in enumerate([current_result] + previous_results[:4]):  # Current + last 4
        if result.success and result.stats:
            comparison_data.append({
                "è¿è¡Œ": "å½“å‰" if i == 0 else f"å†å²-{i}",
                "æ€»æ—¶é—´": result.total_time,
                "å®ä½“æ•°": result.stats.get('entity_count', 0),
                "ä¸‰å…ƒç»„æ•°": result.stats.get('triple_count', 0),
                "é€šè¿‡æ•°": result.stats.get('approved_triples', 0),
                "é€šè¿‡ç‡": result.stats.get('approval_rate', 0)
            })
    
    if comparison_data:
        df = pd.DataFrame(comparison_data)
        st.dataframe(df, use_container_width=True, hide_index=True)
        
        # Performance trends
        if len(comparison_data) > 1 and PLOTLY_AVAILABLE:
            fig = px.line(
                df, 
                x="è¿è¡Œ", 
                y=["æ€»æ—¶é—´", "é€šè¿‡ç‡"],
                title="æ€§èƒ½è¶‹åŠ¿",
                labels={"value": "æ•°å€¼", "variable": "æŒ‡æ ‡"}
            )
            st.plotly_chart(fig, use_container_width=True)
        elif len(comparison_data) > 1:
            st.info("ğŸ“Š è¶‹åŠ¿å›¾éœ€è¦å®‰è£… Plotly åº“: `pip install plotly`")